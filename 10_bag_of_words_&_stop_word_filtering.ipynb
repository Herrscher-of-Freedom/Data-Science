{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 10 Mengenal Text Processing: <br/>Bag of Words & Stop Word Filtering\\\n",
    "\n",
    "Sumber : https://www.youtube.com/watch?v=U30sF4m0bd0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of Words model sebagai representasi text\n",
    "\n",
    "Bag of Words menyederhanakan representasi text sebagai sekumpulan kata serta mengabaikan grammar dan posisi tiap kata pada kalimat. Text akan dikonversi menjadi lowercase dan tanda baca akan diabaikan.\n",
    "\n",
    "Referensi: [https://en.wikipedia.org/wiki/Bag-of-words_model](https://en.wikipedia.org/wiki/Bag-of-words_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yang kita perlukan pertama-tama adalah dataset text yang berisi kalimat-kalimat pendek. Dataset text ini disebut corpus dan dalam dataset berisi 3 kalimat pendek di dalamnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Linux has been around since the mid-1990s.',\n",
    "    'Linux distributions include the Linux kernel.',\n",
    "    'Linux is one of the most prominent open-source software.'\n",
    "]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bag of Words model dengan `CountVectorizer`\n",
    "\n",
    "Bag of Words model dapat diterapkan dengan memanfatkan `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah-langkah penerapan:\n",
    "\n",
    "- Mengimport modul CountVectorizer\n",
    "- Membuat objek dari class CountVectorizer dengan memanggil CountVectorizer() dan ditampung ke dalam variable 'vectorizer'\n",
    "- Kemudian objek 'vectorizer' akan digunakan untuk melakukan metode fit_transform terhadap dataset corpus.\n",
    "- Hasilnya akan dikonversikan ke dalam array dengan menggunakan method todense() dan hasilnya akan dimasukkan ke dalam variable 'vectorized_X'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena teknik yang digunakan adalah Bag of Words, maka hasil output merupakan sekumpulan kata yang berada di dalam keranjang/bags dan terururt secara alfabetical dan menjadi lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika diperhatikan dengan matrix yang diatas, angka pertama adalah 1 yang artinya kata '1990s' ada dalam kalimat tersebut. Apabila angkanya 0 maka kata tersebut tidak ada di dalam kalimat tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'around',\n",
       " 'been',\n",
       " 'distributions',\n",
       " 'has',\n",
       " 'include',\n",
       " 'is',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'most',\n",
       " 'of',\n",
       " 'one',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'since',\n",
       " 'software',\n",
       " 'source',\n",
       " 'the']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Euclidean Distance untuk mengukur kedekatan/jarak antar dokumen (vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan representasi Bag of Words suatu algoritma machine learning dapat lebih mudah mengukur kemiripan antar dokumen. Pada tahap ini saya akan menggunakan eculidean distance untuk mengukur kedekatan antar dokumen.\n",
    "\n",
    "Langkah-langkah:\n",
    "\n",
    "- Import modul euclidean_distances\n",
    "-  Melakukan pengukuran jarak antar kalimatnya (kalimat pertama akan diukur kedekatannya dengan kalimat kedua dan kalimat pertama akan diukur kedekatannya dengan kalimat ketiga, kalimat kedua akan diukur kedekatannya dengan kalimat ketiganya).\n",
    "- Untuk mengukur kedekatannya kita dapat menggunakan for loop statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarak dokumen 1 dan 2: [[3.16227766]]\n",
      "Jarak dokumen 1 dan 3: [[3.74165739]]\n",
      "Jarak dokumen 2 dan 3: [[3.46410162]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "for i in range(len(vectorized_X)):\n",
    "    for j in range(i, len(vectorized_X)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        jarak = euclidean_distances(vectorized_X[i], vectorized_X[j])\n",
    "        print(f'Jarak dokumen {i+1} dan {j+1}: {jarak}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dapat dilihat hasil output diatas bahwa jarak terjauh terdapat pada dokumen 1 dan 2 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stop Word Filtering pada text\n",
    "\n",
    "Stop Word Filtering menyederhanakan representasi text dengan mengabaikan beberapa kata seperti determiners (the, a, an),  auxiliary verbs (do, be, will), dan prepositions (on, in, at).\n",
    "\n",
    "Referensi: [https://en.wikipedia.org/wiki/Stop_word](https://en.wikipedia.org/wiki/Stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset yang kita gunakan adalah dataset Corpus. \n",
    "\n",
    "Dataset corpus ini terdapat kata-kata seperti has, the, been, is, dan of. Kata-kata ini akan menjadi stop word/kata-kata yang akan diabaikan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the Linux kernel.',\n",
       " 'Linux is one of the most prominent open-source software.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stop Word Filtering dengan `CountVectorizer`\n",
    "\n",
    "Stop Word Filtering juga dapat diterapkan dengan memanfatkan `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah-langkah penerapan :\n",
    "\n",
    "- Import module CountVectorizer\n",
    "- Kita akan membentuk objek dari class CountVectorizer dengan menggunakan parameter stop_words yang berisi 'english', karena  stop wordsnya yang diinginkan adalah dalam bentuk bahasa inggris. Objek ini akan ditampung ke dalam variable 'vectorizer'\n",
    "- Kemudian 'vectorizer' ini akan digunakan untuk melakukan fit_transform terhadap dataset corpus dan dikonversikan ke dalam array 2 dimensi dengan menggunakan todense(). \n",
    "- Hasilnya akan ditampung ke dalam variable 'vectorized_X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 2, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'distributions',\n",
       " 'include',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'software',\n",
       " 'source']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dapat dilihat dari hasil output terdapat beberapa kata yang hilang berarti dapat disimpulkan bahwa kita berhasil untuk menggunakan metode stop words."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
